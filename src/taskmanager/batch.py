"""
Enhanced batch job manager with proper SLURM job submission
"""

import os
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional
from .config import SlurmConfig
from .job_parser import JobParser
from .utils import TaskManagerError

class BatchManager:
    def __init__(self, config: SlurmConfig):
        self.config = config
        
    def generate_batch_script(self, jobs: List[Dict[str, Any]], output_file: str = "batch_job.sh", 
                            execution_mode: str = "sequential", dry_run: bool = False) -> str:
        """Generate complete batch submission script"""
        
        if execution_mode == "sequential":
            return self._generate_sequential_batch(jobs, output_file, dry_run)
        else:
            return self._generate_parallel_batch(jobs, output_file, dry_run)
    
    def _generate_sequential_batch(self, jobs: List[Dict[str, Any]], output_file: str, dry_run: bool) -> str:
        """Generate script that submits jobs with dependencies"""
        
        script_lines = [
            "#!/bin/bash",
            "# SLURM Batch Job Submission Script",
            "# Generated by TaskManager",
            "",
            "set -euo pipefail",
            "",
            "# Configuration",
            "DRY_RUN=" + ("true" if dry_run else "false"),
            "VERBOSE=true",
            "",
            "# Colors for output",
            "RED='\\033[0;31m'",
            "GREEN='\\033[0;32m'",
            "YELLOW='\\033[1;33m'",
            "NC='\\033[0m' # No Color",
            "",
            "log_info() { echo -e \"${GREEN}[INFO]${NC} $1\"; }",
            "log_warn() { echo -e \"${YELLOW}[WARN]${NC} $1\"; }",
            "log_error() { echo -e \"${RED}[ERROR]${NC} $1\"; }",
            "",
            "# Function to submit a job step",
            "submit_job_step() {",
            "    local script_path=\"$1\"",
            "    local dependency=\"$2\"",
            "    local job_type=\"$3\"",
            "    local nodes=\"$4\"",
            "    ",
            "    if [[ ! -f \"$script_path\" ]]; then",
            "        log_error \"Script not found: $script_path\"",
            "        return 1",
            "    fi",
            "",
            "    # Make script executable",
            "    chmod +x \"$script_path\"",
            "",
            "    # Generate SLURM submission script",
            "    local submit_script=\"submit_$(basename \"$script_path\" .sh).sh\"",
            "    generate_slurm_script \"$script_path\" \"$submit_script\" \"$job_type\" \"$nodes\"",
            "",
            "    # Build sbatch command",
            "    local sbatch_cmd=\"sbatch\"",
            "    if [[ -n \"$dependency\" ]]; then",
            "        sbatch_cmd=\"$sbatch_cmd --dependency=afterok:$dependency\"",
            "    fi",
            "    sbatch_cmd=\"$sbatch_cmd $submit_script\"",
            "",
            "    if [[ \"$DRY_RUN\" == \"true\" ]]; then",
            "        log_info \"Would execute: $sbatch_cmd\"",
            "        echo \"fake_job_id_$(date +%s)\"",
            "    else",
            "        log_info \"Submitting: $script_path\"",
            "        local job_id=$(eval \"$sbatch_cmd\" | grep -o '[0-9]\\+' | head -1)",
            "        if [[ -n \"$job_id\" ]]; then",
            "            log_info \"Submitted job ID: $job_id\"",
            "            echo \"$job_id\"",
            "        else",
            "            log_error \"Failed to submit $script_path\"",
            "            return 1",
            "        fi",
            "    fi",
            "}",
            "",
            "# Function to generate SLURM wrapper script",
            "generate_slurm_script() {",
            "    local original_script=\"$1\"",
            "    local slurm_script=\"$2\"",
            "    local job_type=\"$3\"",
            "    local nodes=\"$4\"",
            "",
            "    cat > \"$slurm_script\" << 'EOF'",
        ]
        
        # Add SLURM headers using the first job's config
        first_job = jobs[0] if jobs else {}
        job_type = first_job.get('job_type', 'production')
        nodes = first_job.get('nodes', 1)
        
        # Get SLURM headers
        headers = self.config.format_sbatch_headers(job_type, nodes).split('\n')
        script_lines.extend(headers)
        script_lines.extend([
            "",
            "# Change to job directory",
            "cd \"$SLURM_SUBMIT_DIR\"",
            "",
            "# Load modules",
            "module purge",
            "module load gromacs/2024.3-gcc-14.2.0",
            "",
            "# Execute the actual script",
            "bash \"$1\"",
            "EOF",
            "",
            "    # Replace placeholder with actual script path",
            "    sed -i \"s|\\$1|$original_script|g\" \"$slurm_script\"",
            "}",
            "",
            "# Main submission logic",
            "echo \"=== SLURM Batch Job Submission ===\"",
            "echo \"Execution mode: sequential\"",
            f"echo \"Dry run: {'yes' if dry_run else 'no'}\"",
            "echo",
            "",
            "prev_job_id=\"\"",
            ""
        ])
        
        # Add job submission commands
        for job in jobs:
            job_name = job.get('name', 'unknown')
            job_type = job.get('job_type', 'production')
            nodes = job.get('nodes', 1)
            scripts = job.get('scripts', [])
            path = job.get('path', '.')
            
            script_lines.append(f"# Job: {job_name}")
            
            for script in scripts:
                script_path = f"{path}/{script}"
                script_lines.extend([
                    f"log_info \"Submitting {script}...\"",
                    f"job_id=$(submit_job_step \"{script_path}\" \"$prev_job_id\" \"{job_type}\" \"{nodes}\")",
                    "if [[ $? -eq 0 ]]; then",
                    "    prev_job_id=\"$job_id\"",
                    f"    log_info \"Queued {script} with job ID: $job_id\"",
                    "else",
                    f"    log_error \"Failed to submit {script}\"",
                    "    exit 1",
                    "fi",
                    ""
                ])
        
        script_lines.extend([
            "if [[ \"$DRY_RUN\" == \"true\" ]]; then",
            "    log_info \"Dry run completed. No jobs were actually submitted.\"",
            "else",
            "    log_info \"All jobs submitted successfully!\"",
            "    log_info \"Monitor with: squeue -u $USER\"",
            "    log_info \"Check logs in: logs/\"",
            "fi"
        ])
        
        # Write the batch script
        with open(output_file, 'w') as f:
            f.write('\n'.join(script_lines))
        
        os.chmod(output_file, 0o755)
        return output_file
    
    def _generate_parallel_batch(self, jobs: List[Dict[str, Any]], output_file: str, dry_run: bool) -> str:
        """Generate script that submits all jobs in parallel"""
        
        script_lines = [
            "#!/bin/bash",
            "# SLURM Parallel Job Submission Script",
            "",
            "set -euo pipefail",
            "",
            "echo \"=== SLURM Parallel Job Submission ===\"",
            ""
        ]
        
        job_ids = []
        for i, job in enumerate(jobs):
            job_name = job.get('name', f'job_{i}')
            job_type = job.get('job_type', 'production')
            nodes = job.get('nodes', 1)
            scripts = job.get('scripts', [])
            path = job.get('path', '.')
            
            for script in scripts:
                script_path = f"{path}/{script}"
                job_var = f"job_id_{i}_{script.replace('.', '_').replace('-', '_')}"
                job_ids.append(job_var)
                
                if dry_run:
                    script_lines.append(f"echo \"Would submit: {script_path}\"")
                else:
                    script_lines.extend([
                        f"echo \"Submitting {script_path}...\"",
                        f"{job_var}=$(sbatch --parsable {script_path})",
                        f"echo \"Job {script}: ${job_var}\""
                    ])
        
        if not dry_run:
            script_lines.extend([
                "",
                "echo \"All jobs submitted. Job IDs:\"",
                *[f"echo \"  {job_id}: ${job_id}\"" for job_id in job_ids]
            ])
        
        with open(output_file, 'w') as f:
            f.write('\n'.join(script_lines))
        
        os.chmod(output_file, 0o755)
        return output_file

    def generate_script(self, jobs: List[Dict[str, Any]], execution_mode: str = "sequential") -> str:
        """Generate batch script with job-specific resources"""
        script_parts = []
        
        # Add SLURM headers for the batch script itself
        batch_params = self.config.get_global_params()
        script_parts.extend([
            "#!/bin/bash",
            "",
            "# SLURM directives for workflow manager job"
        ])
        
        # Add SLURM headers
        sbatch_options = self.config.format_sbatch_options('workflow')
        script_parts.extend([f"#SBATCH {opt}" for opt in sbatch_options])
        
        script_parts.extend([
            "",
            "# Generated SLURM batch script",
            f"# Configuration: {self.config.config_file}",
            f"# Execution mode: {execution_mode}",
            "",
            "set -euo pipefail",
            "",
            "# Ensure output directory exists",
            "mkdir -p logs",
            "",
            "# Function to submit job steps",
            "submit_job_step() {",
            "    local job_name=\"$1\"",
            "    local script_path=\"$2\"", 
            "    local dependency=\"$3\"",
            "    ",
            "    local sbatch_args=\"\"",
            "    if [[ -n \"$dependency\" ]]; then",
            "        sbatch_args=\"--dependency=afterok:$dependency\"",
            "    fi",
            "    ",
            "    echo \"Submitting: $job_name ($script_path)\"",
            "    if [[ -n \"$dependency\" ]]; then",
            "        echo \"  Dependency: $dependency\"",
            "    fi",
            "    ",
            "    # Submit the job and capture job ID",
            "    local job_output=$(sbatch $sbatch_args \"$script_path\" 2>&1)",
            "    local exit_code=$?",
            "    ",
            "    if [[ $exit_code -eq 0 ]]; then",
            "        # Extract job ID from sbatch output (format: \"Submitted batch job 12345\")",
            "        local job_id=$(echo \"$job_output\" | grep -oE '[0-9]+$')",
            "        echo \"  Job ID: $job_id\"",
            "        echo \"$job_id\"",
            "    else",
            "        echo \"  ERROR: Failed to submit job\"",
            "        echo \"  Output: $job_output\"",
            "        exit 1",
            "    fi",
            "}",
            "",
            "# Job execution starts here",
            "echo \"Starting workflow execution...\"",
            "echo \"Mode: $execution_mode\"",
            "echo",
            "",
            "prev_job_id=''",
            ""
        ])
        
        # Process each job
        for job in jobs:
            job_name = job['name']
            scripts = job.get('scripts', [])
            
            if not scripts:
                continue
            
            script_parts.extend([
                f"# === {job_name.upper()} ===",
            ])
            
            # Add chunking info if applicable
            if job.get('is_chunked', False):
                chunk_meta = job.get('chunk_metadata', {})
                total_chunks = chunk_meta.get('total_chunks', 1)
                chunk_length = chunk_meta.get('chunk_length_ns', 1)
                total_time = total_chunks * chunk_length
                script_parts.append(f"# Chunked simulation: {total_chunks} chunks × {chunk_length} ns = {total_time} ns total")
            
            # Submit job scripts
            for i, script in enumerate(scripts):
                script_path = f"{job['path']}/{script}"
                step_name = f"{job_name}_{script.replace('.sh', '')}"
                
                if execution_mode == "sequential":
                    script_parts.extend([
                        f"echo \"Step {i+1}/{len(scripts)}: {script}\"",
                        f"job_id=$(submit_job_step \"{step_name}\" \"{script_path}\" \"$prev_job_id\")",
                        "prev_job_id=\"$job_id\"",
                        "echo",
                    ])
                elif execution_mode == "parallel":
                    script_parts.extend([
                        f"echo \"Step {i+1}/{len(scripts)}: {script} (parallel)\"",
                        f"job_id=$(submit_job_step \"{step_name}\" \"{script_path}\" \"\")",
                        f"job_ids_{job_name}+=(\"$job_id\")",
                        "echo",
                    ])
            
            script_parts.append("")
        
        # Add completion message
        script_parts.extend([
            "echo \"All jobs submitted successfully!\"",
            "echo \"Monitor with: squeue -u $USER\"",
            ""
        ])
        
        return '\n'.join(script_parts)