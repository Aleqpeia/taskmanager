"""
Enhanced batch job manager with proper SLURM job submission
"""

import os
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional
from .config import SlurmConfig
from .job_parser import JobParser
from .utils import TaskManagerError

class BatchManager:
    def __init__(self, config: SlurmConfig):
        """Initialize batch manager with SLURM configuration"""
        if not isinstance(config, SlurmConfig):
            raise TypeError("config must be a SlurmConfig instance")
        self.config = config
        
    def generate_batch_script(self, jobs: List[Dict[str, Any]], output_file: str = "batch_job.sh", 
                            execution_mode: str = "sequential", dry_run: bool = False) -> str:
        """Generate complete batch submission script"""
        
        if execution_mode == "sequential":
            return self._generate_sequential_batch(jobs, output_file, dry_run)
        else:
            return self._generate_parallel_batch(jobs, output_file, dry_run)
    
    def _generate_sequential_batch(self, jobs: List[Dict[str, Any]], output_file: str, dry_run: bool) -> str:
        """Generate script that submits jobs with dependencies"""
        
        script_lines = [
            "#!/bin/bash",
            "# SLURM Batch Job Submission Script",
            "# Generated by TaskManager",
            "",
            "set -euo pipefail",
            "",
            "# Configuration",
            "DRY_RUN=" + ("true" if dry_run else "false"),
            "VERBOSE=true",
            "",
            "# Colors for output",
            "RED='\\033[0;31m'",
            "GREEN='\\033[0;32m'",
            "YELLOW='\\033[1;33m'",
            "NC='\\033[0m' # No Color",
            "",
            "log_info() { echo -e \"${GREEN}[INFO]${NC} $1\"; }",
            "log_warn() { echo -e \"${YELLOW}[WARN]${NC} $1\"; }",
            "log_error() { echo -e \"${RED}[ERROR]${NC} $1\"; }",
            "",
            "# Function to submit a job step",
            "submit_job_step() {",
            "    local script_path=\"$1\"",
            "    local dependency=\"$2\"",
            "    local job_type=\"$3\"",
            "    local nodes=\"$4\"",
            "    ",
            "    if [[ ! -f \"$script_path\" ]]; then",
            "        log_error \"Script not found: $script_path\"",
            "        return 1",
            "    fi",
            "",
            "    # Make script executable",
            "    chmod +x \"$script_path\"",
            "",
            "    # Generate SLURM submission script",
            "    local submit_script=\"submit_$(basename \"$script_path\" .sh).sh\"",
            "    generate_slurm_script \"$script_path\" \"$submit_script\" \"$job_type\" \"$nodes\"",
            "",
            "    # Build sbatch command",
            "    local sbatch_cmd=\"sbatch\"",
            "    if [[ -n \"$dependency\" ]]; then",
            "        sbatch_cmd=\"$sbatch_cmd --dependency=afterok:$dependency\"",
            "    fi",
            "    sbatch_cmd=\"$sbatch_cmd $submit_script\"",
            "",
            "    if [[ \"$DRY_RUN\" == \"true\" ]]; then",
            "        log_info \"Would execute: $sbatch_cmd\"",
            "        echo \"fake_job_id_$(date +%s)\"",
            "    else",
            "        log_info \"Submitting: $script_path\"",
            "        local job_id=$(eval \"$sbatch_cmd\" | grep -o '[0-9]\\+' | head -1)",
            "        if [[ -n \"$job_id\" ]]; then",
            "            log_info \"Submitted job ID: $job_id\"",
            "            echo \"$job_id\"",
            "        else",
            "            log_error \"Failed to submit $script_path\"",
            "            return 1",
            "        fi",
            "    fi",
            "}",
            "",
            "# Function to generate SLURM wrapper script",
            "generate_slurm_script() {",
            "    local original_script=\"$1\"",
            "    local slurm_script=\"$2\"",
            "    local job_type=\"$3\"",
            "    local nodes=\"$4\"",
            "",
            "    cat > \"$slurm_script\" << 'EOF'",
        ]
        
        # Add SLURM headers using the first job's config
        first_job = jobs[0] if jobs else {}
        job_type = first_job.get('job_type', 'production')
        nodes = first_job.get('nodes', 1)
        
        # Get SLURM headers
        headers = self.config.format_sbatch_headers(job_type, nodes).split('\n')
        script_lines.extend(headers)
        script_lines.extend([
            "",
            "# Change to job directory",
            "cd \"$SLURM_SUBMIT_DIR\"",
            "",
            "# Load modules",
            "module purge",
            "module load gromacs/2024.3-gcc-14.2.0",
            "",
            "# Execute the actual script",
            "bash \"$1\"",
            "EOF",
            "",
            "    # Replace placeholder with actual script path",
            "    sed -i \"s|\\$1|$original_script|g\" \"$slurm_script\"",
            "}",
            "",
            "# Main submission logic",
            "echo \"=== SLURM Batch Job Submission ===\"",
            "echo \"Execution mode: sequential\"",
            f"echo \"Dry run: {'yes' if dry_run else 'no'}\"",
            "echo",
            "",
            "prev_job_id=\"\"",
            ""
        ])
        
        # Add job submission commands
        for job in jobs:
            job_name = job.get('name', 'unknown')
            job_type = job.get('job_type', 'production')
            nodes = job.get('nodes', 1)
            scripts = job.get('scripts', [])
            path = job.get('path', '.')
            
            script_lines.append(f"# Job: {job_name}")
            
            for script in scripts:
                script_path = f"{path}/{script}"
                script_lines.extend([
                    f"log_info \"Submitting {script}...\"",
                    f"job_id=$(submit_job_step \"{script_path}\" \"$prev_job_id\" \"{job_type}\" \"{nodes}\")",
                    "if [[ $? -eq 0 ]]; then",
                    "    prev_job_id=\"$job_id\"",
                    f"    log_info \"Queued {script} with job ID: $job_id\"",
                    "else",
                    f"    log_error \"Failed to submit {script}\"",
                    "    exit 1",
                    "fi",
                    ""
                ])
        
        script_lines.extend([
            "if [[ \"$DRY_RUN\" == \"true\" ]]; then",
            "    log_info \"Dry run completed. No jobs were actually submitted.\"",
            "else",
            "    log_info \"All jobs submitted successfully!\"",
            "    log_info \"Monitor with: squeue -u $USER\"",
            "    log_info \"Check logs in: logs/\"",
            "fi"
        ])
        
        # Write the batch script
        with open(output_file, 'w') as f:
            f.write('\n'.join(script_lines))
        
        os.chmod(output_file, 0o755)
        return output_file
    
    def _generate_parallel_batch(self, jobs: List[Dict[str, Any]], output_file: str, dry_run: bool) -> str:
        """Generate script that submits all jobs in parallel"""
        
        script_lines = [
            "#!/bin/bash",
            "# SLURM Parallel Job Submission Script",
            "",
            "set -euo pipefail",
            "",
            "echo \"=== SLURM Parallel Job Submission ===\"",
            ""
        ]
        
        job_ids = []
        for i, job in enumerate(jobs):
            job_name = job.get('name', f'job_{i}')
            job_type = job.get('job_type', 'production')
            nodes = job.get('nodes', 1)
            scripts = job.get('scripts', [])
            path = job.get('path', '.')
            
            for script in scripts:
                script_path = f"{path}/{script}"
                job_var = f"job_id_{i}_{script.replace('.', '_').replace('-', '_')}"
                job_ids.append(job_var)
                
                if dry_run:
                    script_lines.append(f"echo \"Would submit: {script_path}\"")
                else:
                    script_lines.extend([
                        f"echo \"Submitting {script_path}...\"",
                        f"{job_var}=$(sbatch --parsable {script_path})",
                        f"echo \"Job {script}: ${job_var}\""
                    ])
        
        if not dry_run:
            script_lines.extend([
                "",
                "echo \"All jobs submitted. Job IDs:\"",
                *[f"echo \"  {job_id}: ${job_id}\"" for job_id in job_ids]
            ])
        
        with open(output_file, 'w') as f:
            f.write('\n'.join(script_lines))
        
        os.chmod(output_file, 0o755)
        return output_file

    def generate_script(self, jobs: List[Dict[str, Any]], execution_mode: str = "sequential") -> str:
        """Generate batch script that executes jobs directly (not submits them)"""
        script_parts = []
        
        # Add SLURM headers for the workflow job
        script_parts.extend([
            "#!/bin/bash",
            "",
            "# SLURM directives for the workflow job"
        ])
        
        # Add SLURM headers
        sbatch_options = self.config.format_sbatch_options('workflow')
        script_parts.extend([f"#SBATCH {opt}" for opt in sbatch_options])
        
        script_parts.extend([
            "",
            "# Generated SLURM workflow script",
            f"# Configuration: {self.config.config_file}",
            f"# Execution mode: {execution_mode}",
            "",
            "set -euo pipefail",
            "",
            "# Ensure output directory exists", 
            "mkdir -p logs",
            "",
            "# Function to execute job steps",
            "execute_job_step() {",
            "    local step_name=\"$1\"",
            "    local script_path=\"$2\"",
            "    local work_dir=\"$3\"",
            "    ",
            "    echo \"========================================\"",
            "    echo \"Executing: $step_name\"",
            "    echo \"Script: $script_path\"",
            "    echo \"Working directory: $work_dir\"",
            "    echo \"Time: $(date)\"",
            "    echo \"========================================\"",
            "    ",
            "    # Change to working directory",
            "    if [[ -n \"$work_dir\" && -d \"$work_dir\" ]]; then",
            "        cd \"$work_dir\"",
            "        echo \"Changed to directory: $(pwd)\"",
            "    fi",
            "    ",
            "    # Execute the script",
            "    if [[ -f \"$script_path\" ]]; then",
            "        chmod +x \"$script_path\"",
            "        ./$script_path",
            "        local exit_code=$?",
            "        if [[ $exit_code -eq 0 ]]; then",
            "            echo \"✓ $step_name completed successfully\"",
            "        else",
            "            echo \"✗ $step_name failed with exit code $exit_code\"",
            "            exit $exit_code",
            "        fi",
            "    else",
            "        echo \"✗ Script not found: $script_path\"",
            "        exit 1",
            "    fi",
            "    ",
            "    echo",
            "}",
            "",
            "# Workflow execution starts here",
            "echo \"Starting workflow execution...\"",
            "echo \"Mode: $execution_mode\"",
            "echo \"Node: $SLURM_JOB_NODELIST\"",
            "echo \"Job ID: $SLURM_JOB_ID\"",
            "echo \"Working directory: $(pwd)\"",
            "echo",
            ""
        ])
        
        # Process each job
        for job_idx, job in enumerate(jobs):
            job_name = job['name']
            scripts = job.get('scripts', [])
            job_path = job.get('path', '.')
            
            if not scripts:
                continue
            
            script_parts.extend([
                f"# === JOB {job_idx + 1}: {job_name.upper()} ===",
            ])
            
            # Add chunking info if applicable
            if job.get('is_chunked', False):
                chunk_meta = job.get('chunk_metadata', {})
                total_chunks = chunk_meta.get('total_chunks', 1)
                chunk_length = chunk_meta.get('chunk_length_ns', 1)
                total_time = total_chunks * chunk_length
                script_parts.append(f"# Chunked simulation: {total_chunks} chunks × {chunk_length} ns = {total_time} ns total")
            
            script_parts.append("")
            
            # Execute job scripts
            for i, script in enumerate(scripts):
                step_name = f"{job_name}_{script.replace('.sh', '')}"
                
                if execution_mode == "sequential":
                    script_parts.extend([
                        f"echo \"Step {i+1}/{len(scripts)}: {script}\"",
                        f"execute_job_step \"{step_name}\" \"{script}\" \"{job_path}\"",
                        ""
                    ])
                elif execution_mode == "parallel":
                    # For parallel execution, run in background
                    script_parts.extend([
                        f"echo \"Step {i+1}/{len(scripts)}: {script} (parallel)\"",
                        f"execute_job_step \"{step_name}\" \"{script}\" \"{job_path}\" &",
                        f"pids+=($!)",
                        ""
                    ])
            
            # Wait for parallel jobs to complete
            if execution_mode == "parallel":
                script_parts.extend([
                    "# Wait for all parallel jobs in this stage to complete",
                    "for pid in \"${pids[@]}\"; do",
                    "    wait $pid",
                    "    if [[ $? -ne 0 ]]; then",
                    "        echo \"Parallel job failed\"",
                    "        exit 1",
                    "    fi",
                    "done",
                    "pids=()",
                    ""
                ])
            
            script_parts.append("")
        
        # Add completion message
        script_parts.extend([
            "echo \"========================================\"",
            "echo \"All workflow steps completed successfully!\"", 
            "echo \"Completion time: $(date)\"",
            "echo \"========================================\"",
            ""
        ])
        
        return '\n'.join(script_parts)